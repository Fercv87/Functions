import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from statsmodels.formula.api import ols
from statsmodels.stats.diagnostic import het_breuschpagan, het_white, het_goldfeldquandt, acorr_breusch_godfrey
from statsmodels.stats.stattools import durbin_watson
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from scipy import stats
from scipy.stats import f, kstest, norm, anderson, chi2

def linear_regression_analysis_v3(df, X_columns, y_column, plot_graphs=True):
    X = df[X_columns]
    y = df[y_column]
####################################################################################################
    # --- Scikit-learn Linear Regression ---
    print("---- Scikit-learn Linear Regression --------------------------------------------\n")
    
    # Initialize the Linear Regression model
    lr_model = LinearRegression()
    lr_model.fit(X, y)
    y_pred = lr_model.predict(X)
    df_plot = pd.DataFrame({'Actual': y.reset_index(drop=True), 'Predicted': y_pred})
    residuals = y - y_pred    
    # Model Fit Summary
    print_model_fit_summary(lr_model, X, y, y_pred)
####################################################################################################
    print("---- Linear Regression Graphical Analysis --------------------------------------------\n")
    # Graphical Analysis
    plot_graphical_analysis(X, y, y_pred,df_plot,residuals, plot_graphs)

####################################################################################################
    # --- Statsmodels Linear Regression ---
    print("\n---- Statsmodels Linear Regression --------------------------------------------\n")
    model = fit_statsmodels_ols(X, y)
    exog = model.model.exog
    
    # Final Model Diagnostics Summary
    print_final_diagnostics_summary(model, y_pred, residuals,X, X_const, exog)
    
    # Return the models for further analysis if needed
    return lr_model, model
 ####################################################################################################
def print_model_fit_summary(lr_model, X, y, y_pred):
    # Calculate statistics
    r2 = r2_score(y, y_pred)
    adjusted_r2 = 1 - (1-r2)*(len(y)-1)/(len(y)-X.shape[1]-1)
    mae = mean_absolute_error(y, y_pred)
    rmse = np.sqrt(mean_squared_error(y, y_pred))
    
    # Print model coefficients and intercept
    print("Intercept (Bias):", lr_model.intercept_)
    print("Coefficients:")
    for feature, coef in zip(X.columns, lr_model.coef_):
        print(f"{feature}: {coef}")
    
    # Print the statistics
    print("R-squared (R2):", r2)
    print("Adjusted R-squared:", adjusted_r2)
    print("Mean Absolute Error (MAE):", mae)
    print("Root Mean Squared Error (RMSE):", rmse)
####################################################################################################
def plot_graphical_analysis(X, y, y_pred,df_plot,residuals, plot_graphs=True):
    if plot_graphs:
        #Seleccionamos tanto el estilo como el contexto para el gráfico
        sns.set_style("darkgrid")
        sns.set_context("paper")    

        # Create a DataFrame to hold the predictors, actual and predicted values
        df_pair = X.copy()
        df_pair['Actual'] = y
        df_pair['Predicted'] = y_pred
        
        # Plot pair plot
        sns.pairplot(df_pair)
        plt.suptitle("Pair Plot of Predictors, Actual and Predicted Values", y=1.02)
        plt.show()

        # Melting the DataFrame for sns.pairplot with both actual and predicted values
        df_long = pd.melt(df_pair, id_vars=X.columns.tolist(), var_name='Type', value_name='Output')
        # Creating the pair plot with custom settings
        pair_plot = sns.pairplot(df_long, hue='Type', palette='Set1',
                                 markers=["o", "s"], plot_kws={'alpha': 0.6}, diag_kind='kde')
        # Adding regression lines to the upper triangle plots
        for i, j in zip(*np.triu_indices_from(pair_plot.axes, 1)):
            sns.regplot(x=df_pair.columns[j], y=df_pair.columns[i], data=df_pair,
                        ax=pair_plot.axes[i, j], scatter=False, color='blue')
        # Enhancing the aesthetics
        pair_plot.fig.suptitle("Enhanced Pair Plot with Actual vs. Predicted Values and Regression Lines", y=1.02)
        plt.show()

         # Start the 2x2 plot grid
        fig, axs = plt.subplots(2, 2, figsize=(15, 12))
        # Plot actual vs predicted values over index using Seaborn
        sns.lineplot(ax=axs[0, 0], data=df_plot)
        axs[0, 0].set_title("Actual vs Predicted Values Over Index")
        axs[0, 0].set_xlabel("Index")
        axs[0, 0].set_ylabel("Values")
        # Plot predicted vs actual values
        sns.scatterplot(ax=axs[0, 1], x=y, y=y_pred, alpha=0.5)
        axs[0, 1].plot([min(y), max(y)], [min(y), max(y)], '--', color='red')  # Diagonal line
        axs[0, 1].set_title("Actual vs Predicted Values")
        axs[0, 1].set_xlabel("Actual Values")
        axs[0, 1].set_ylabel("Predicted Values")
        # Plot histogram of residuals
        sns.histplot(ax=axs[1, 0], data=residuals, bins=20, kde=False, edgecolor='k')
        axs[1, 0].set_title("Distribution of Residuals")
        axs[1, 0].set_xlabel("Residual")
        axs[1, 0].set_ylabel("Frequency")
        # Plot residuals in a Q-Q plot to check for normality
        ax_qq = axs[1, 1]
        sm.qqplot(residuals, line='s', ax=ax_qq)  # 's' means standard line
        ax_qq.set_title("Q-Q Plot of Residuals")
        ax_qq.set_xlabel("Theoretical Quantiles")
        ax_qq.set_ylabel("Sample Quantiles")
        # Adjust layout
        plt.tight_layout()
        plt.show()
        # Plot residuals
        for col in X.columns:
            plt.figure(figsize=(10, 6))
            sns.scatterplot(x=X[col], y=residuals, alpha=0.5)
            plt.axhline(y=0, color='red', linestyle='--')
            plt.title(f"Residuals vs {col}")
            plt.xlabel(col)
            plt.ylabel("Residual")
            plt.show()
####################################################################################################
def fit_statsmodels_ols(X, y):
        # Add a constant to the predictors
        X_const = sm.add_constant(X)
        # Fit the model
        return sm.OLS(y, X_const).fit()
####################################################################################################
def print_final_diagnostics_summary(model, y_pred, residuals,X, X_const, exog):
    # Get and print the summary
    summary = model.summary()
    print(summary)
    
    # Extracting and interpreting key statistics
    dfn, dfd = model.df_model, model.df_resid  # degrees of freedom for the model and residuals
    f_statistic = model.fvalue
    f_pvalue = model.f_pvalue
    coefficients = model.params
    t_values = model.tvalues
    p_values = model.pvalues
    conf_int = model.conf_int()
    


    # Interpretation
    print("\n---- Interpretation: --------------------------------------------\n")

    # F-statistic and Prob (F-statistic)
    print("\nF-statistic tests the joint significance of all regression coefficients")
    print("# Null hypothesis: All coefficients are equal to zero")
    print("# Alternative hypothesis: At least one coefficient is not equal to zero")
    print("\nF-statistic:", f_statistic)

    print("Prob (F-statistic):", f_pvalue)
    if f_pvalue < 0.05:
        print("The model is statistically significant (p < 0.05) and a good fit to the data.")
    else:
        print("The model is not statistically significant (p >= 0.05) and may not be a good fit to the data.")
    # Plot for this test: F-statistic and Prob (F-statistic)
    x = np.linspace(f.ppf(0.0001, dfn, dfd), f.ppf(0.9999, dfn, dfd), 100)
    plt.plot(x, f.pdf(x, dfn, dfd), label='F-Distribution')
    # Calculate the critical value threshold for alpha = 0.05
    alpha = 0.05
    f_crit_value = f.ppf(1 - alpha, dfn, dfd)
    # Highlight the critical region
    plt.fill_between(x, 0, f.pdf(x, dfn, dfd), where=x >= f_crit_value, color='red', alpha=0.5, label='Critical Region (Rejection Region)')
    # Plot the F-statistic
    plt.axvline(f_statistic, color='black', linestyle='dashed', label=f'F-Statistic = {f_statistic:.2f}')
    plt.xlabel('F-Statistic')
    plt.ylabel('Probability Density')
    plt.title('F-Test for Regression Model')
    plt.legend()
    plt.show()
    
    # Coefficients, t-values, p-values, and Confidence Intervals
    print("\nCoefficients, t-values, p-values, and Confidence Intervals:")
    print("\n# T-tests for individual coefficients")
    print("Null hypothesis (H0): coefficient = 0")
    print("Alternative hypothesis (H1): coefficient != 0")
    for i, (col, coef, t, p) in enumerate(zip(model.model.exog_names, coefficients, t_values, p_values)):
        conf = conf_int.iloc[i]  # Get the confidence interval for the ith predictor
        print(f"\nPredictor: {col}")
        print(f"Coefficient: {coef}")
        print(f"T-value: {t}")
        print(f"P-value: {p}")
        print(f"95% Confidence Interval: {conf.tolist()}")  # Convert to list for display


        # Interpretation
        if p < 0.05:
            print("The predictor is statistically significant (p < 0.05).")
            if conf[0] * conf[1] > 0:
                print("The 95% CI does not include 0, suggesting a significant relationship with the response variable.")
        else:
            print("The predictor is not statistically significant (p >= 0.05).")

        # Visualization of the t-test for this coefficient
        plt.figure(figsize=(10, 2))
        t_dist = np.linspace(stats.t.ppf(0.001, model.df_resid), stats.t.ppf(0.999, model.df_resid), 100)
        plt.plot(t_dist, stats.t.pdf(t_dist, model.df_resid), label='T-Distribution')
        # Confidence Interval
        plt.axvline(conf[0], color='grey', linestyle='dotted', label='95% CI')
        plt.axvline(conf[1], color='grey', linestyle='dotted')
        # T-statistic
        plt.axvline(t, color='black', linestyle='dashed', label=f'T-Statistic = {t:.2f}')
        # Critical t values for two-tailed test at alpha = 0.05
        crit_t_value_left = stats.t.ppf(0.025, model.df_resid)
        crit_t_value_right = stats.t.ppf(0.975, model.df_resid)
        plt.fill_between(t_dist, 0, stats.t.pdf(t_dist, model.df_resid), where=(t_dist <= crit_t_value_left) | (t_dist >= crit_t_value_right), color='red', alpha=0.3, label='Rejection Region')
        plt.title(f'T-Test for {col}')
        plt.xlabel('t-Statistic')
        plt.ylabel('Probability Density')
        plt.legend()
        plt.tight_layout()
        plt.show()          

    print("\n---- Testing Model Assumptions ----------------------------------------------------------------------------\n")
    
    ####### 1 Linearity Test
    print("\n1. Linearity:")
    print("To check for the linearity assumption, inspect the residual plots. \nLook for systematic patterns when plotting residuals against predicted values or any of the independent variables. \nIf you observe a pattern, such as a clear curve, funnel shape, or other non-random distribution, this may indicate non-linearity.")
    plt.figure(figsize=(6, 4))
    plt.scatter(y_pred, residuals, alpha=0.5)
    plt.axhline(y=0, color='red', linestyle='--')
    plt.title("Residuals vs. Predicted Values")
    plt.xlabel("Predicted Values")
    plt.ylabel("Residuals")
    plt.show()
    print("Solutions may include transforming variables, adding polynomial or interaction terms, or considering non-linear models.")

    ####### 2 Homoscedasticity Test - Breusch-Pagan
    print("\n2. Homoscedasticity (Constant Error Variance):")
    print("\n# Breusch-Pagan test")
    print("Null hypothesis (H0): The error variances are all equal (no heteroscedasticity). Residuals are homoscedastic")
    print("Alternative hypothesis (H1): The error variances are not equal (heteroscedasticity is present). Residuals are heteroscedastic")
    bp_test_statistic, bp_p_value, _, _ = het_breuschpagan(residuals, model.model.exog)
    print("Breusch-Pagan test statistic:", bp_test_statistic)
    print("Breusch-Pagan test p-value:", bp_p_value)
    if bp_p_value < 0.05:
        print("Evidence of heteroscedasticity (p < 0.05), suggesting the variance of the errors is not constant.")
    else:
        print("No evidence of heteroscedasticity (p >= 0.05), suggesting the variance of the errors is constant.")
    
    # Degrees of freedom usually equal to the number of independent variables in the model
    degrees_of_freedom = len(model.model.exog[0]) - 1
    # X values for plotting
    x_values = np.linspace(0, chi2.ppf(0.99, df=degrees_of_freedom), 1000)
    # Y values for plotting
    y_values = chi2.pdf(x_values, df=degrees_of_freedom)
    # Plot the chi-squared distribution
    plt.plot(x_values, y_values, label='Chi-squared Distribution')
    # Plot the test statistic
    plt.axvline(bp_test_statistic, color='red', linestyle='dashed', linewidth=2, label='Test Statistic')
    # Show critical value line for alpha=0.05
    critical_value = chi2.ppf(0.95, df=degrees_of_freedom)
    plt.axvline(critical_value, color='green', linestyle='dashed', linewidth=2, label='Critical Value (α=0.05)')
    # Adding annotations
    plt.annotate(f'Test Statistic = {bp_test_statistic:.4f}', 
                xy=(bp_test_statistic, chi2.pdf(bp_test_statistic, df=degrees_of_freedom)),
                xytext=(bp_test_statistic, chi2.pdf(bp_test_statistic, df=degrees_of_freedom)*1.5),
                arrowprops=dict(facecolor='black', shrink=0.05))
    plt.annotate(f'Critical Value = {critical_value:.4f}', 
                xy=(critical_value, chi2.pdf(critical_value, df=degrees_of_freedom)),
                xytext=(critical_value, chi2.pdf(critical_value, df=degrees_of_freedom)*1.5),
                arrowprops=dict(facecolor='green', shrink=0.05))
    # Additional plot formatting
    plt.xlabel('Value')
    plt.ylabel('Probability Density')
    plt.title('Chi-Squared Distribution with Test Statistic')
    plt.legend()
    plt.grid(True)
    # Show plot
    plt.show()    
        
    # Plotting the residuals vs predicted values
    plt.figure(figsize=(6, 4))
    plt.scatter(y_pred, residuals, alpha=0.5)
    plt.axhline(y=0, color='red', linestyle='--')
    plt.title('Residuals vs. Predicted Values')
    plt.xlabel('Predicted Values')
    plt.ylabel('Residuals')

    plt.show()

    # Homoscedasticity Test - White
    white_test_statistic, white_test_p_value, _, _ = het_white(residuals, model.model.exog)
    print("\n# White test")
    print("Null hypothesis (H0): Homoscedasticity is present. Residuals are homoscedastic")
    print("Alternative hypothesis (H1): Heteroscedasticity is present. Residuals are heteroscedastic")
    print("White test statistic:", white_test_statistic)
    print("White test p-value:", white_test_p_value)
    if white_test_p_value < 0.05:
        print("Evidence of heteroscedasticity according to the White test (p < 0.05), suggesting the variance of the errors is not constant.")
    else:
        print("No evidence of heteroscedasticity according to the White test (p >= 0.05), suggesting the variance of the errors is constant.")

    # Homoscedasticity Test - Goldfeld-Quandt
    print("\nGoldfeld-Quandt Homoscedasticity Test:")
    print("Null Hypothesis (H0): The error variance is constant across the range of data (homoscedasticity)")
    print("Alternative Hypothesis (H1): The error variance changes with the level of an independent variable (heteroscedasticity)")
    gq_test_statistic, gq_p_value, _ = het_goldfeldquandt(residuals, model.model.exog)
    print("GQ test statistic:", gq_test_statistic)
    print("GQ test p-value:", gq_p_value)
    # Interpretation of the results
    if gq_p_value < 0.05:
        print("The null hypothesis of homoscedasticity is rejected, suggesting heteroscedasticity (p < 0.05).")
    else:
        print("The null hypothesis of homoscedasticity cannot be rejected (p >= 0.05), suggesting no heteroscedasticity.")
    
    # Calculate the degrees of freedom
    n = len(residuals)  # Total number of observations
    k = exog.shape[1]   # Number of parameters including the intercept
    # Divide the data into two halves for the Goldfeld-Quandt test
    split = n // 2 if n % 2 == 0 else (n // 2) + 1
    # Assuming the data is sorted in ascending order of the independent variable
    # which is a requirement for the GQ test.
    dfn = split - k  # Degrees of freedom for the first half
    dfd = (n - split) - k  # Degrees of freedom for the second half
    print(f"Degrees of freedom for the first half: {dfn}")
    print(f"Degrees of freedom for the second half: {dfd}")
    # Set up the range of values for the F-distribution
    x = np.linspace(0.01, 5, 1000)
    y = f.pdf(x, dfn, dfd)
    # Plot the F-distribution
    plt.plot(x, y, 'b-', label='F-distribution')
    # Shade the area under the curve beyond the GQ test statistic
    x_fill = np.linspace(gq_test_statistic, 5, 1000)
    y_fill = f.pdf(x_fill, dfn, dfd)
    plt.fill_between(x_fill, 0, y_fill, color='red', alpha=0.5, label='p-value area')
    # Plot a line indicating the GQ test statistic
    plt.axvline(gq_test_statistic, color='black', linestyle='--', label=f'GQ Statistic ({gq_test_statistic})')
    # Annotate the p-value on the plot
    plt.annotate(f'p-value: {gq_p_value:.3f}', xy=(gq_test_statistic, f.pdf(gq_test_statistic, dfn, dfd)), xytext=(gq_test_statistic+0.5, 0.2),
                arrowprops=dict(facecolor='black', arrowstyle='->'))
    # Adding labels and title
    plt.title('Goldfeld-Quandt Homoscedasticity Test')
    plt.xlabel('F-value')
    plt.ylabel('Probability Density')
    plt.legend()
    # Show the plot
    plt.show()
    
    
    # Homoscedasticity Tests Summary and Recommendations
    print("\nHomoscedasticity Tests Summary and Recommendations:")
    # Initialize a flag to indicate if any test indicates heteroscedasticity
    heteroscedasticity_flag = False
    # Summarize the outcomes of the homoscedasticity tests
    if bp_p_value < 0.05:
        print("- Breusch-Pagan test indicates heteroscedasticity (p < 0.05).")
        heteroscedasticity_flag = True
    else:
        print("- Breusch-Pagan test does not indicate heteroscedasticity (p >= 0.05).")
    if white_test_p_value < 0.05:
        print("- White test indicates heteroscedasticity (p < 0.05).")
        heteroscedasticity_flag = True
    else:
        print("- White test does not indicate heteroscedasticity (p >= 0.05).")
    if gq_p_value < 0.05:
        print("- Goldfeld-Quandt test indicates heteroscedasticity (p < 0.05).")
        heteroscedasticity_flag = True
    else:
        print("- Goldfeld-Quandt test does not indicate heteroscedasticity (p >= 0.05).")
    # Provide a final conclusion and suggestions
    if heteroscedasticity_flag:
        print("\nAt least one test suggests heteroscedasticity. This could be an indication that the model assumptions are violated.")
        print("Suggestions to address heteroscedasticity include:")
        print("- Transforming the dependent variable (e.g., log transformation).")
        print("- Using robust standard errors to account for heteroscedasticity.")
        print("- Re-examining the functional form of the model to ensure that all relationships are properly modeled.")
        print("- Checking for outliers that may influence the variance.")
        print("- Considering weighted least squares regression if the errors are proportional to another variable.")
    else:
        print("\nNo test indicates heteroscedasticity. The assumption of constant variance holds for the model.")

    ####### 3 Independent Error Terms Test
    print("\n3. Independent Error Terms:")
    print("\n# Durbin-Watson test for autocorrelation")
    print("Null hypothesis (H0): There is no autocorrelation in the sample.")
    print("Alternative hypothesis (H1): There is autocorrelation in the sample.") 
    dw_statistic = durbin_watson(residuals)
    print('Durbin-Watson statistic:', dw_statistic)
    # Interpretation of the Durbin-Watson statistic
    if dw_statistic < 1.5:
        print("There is evidence of positive autocorrelation (DW <1.5).")
    elif dw_statistic > 2.5:
        print("There is evidence of negative autocorrelation (DW >2.5).")
    else:
        print("There is no evidence of autocorrelation.")
    # ACF plot to visualize autocorrelation
    plt.figure(figsize=(6, 4))
    plot_acf(residuals, alpha=0.05, title='Autocorrelation Function (ACF) of Residuals')
    plt.show()
    # Breusch-Godfrey test for autocorrelation
    print("\nBreusch-Godfrey test for autocorrelation")
    print("Null Hypothesis (H0): There is no autocorrelation of any order up to p")
    print("Alternative Hypothesis (H1): There is autocorrelation")
    bg_test = acorr_breusch_godfrey(model, nlags=1)  # Adjust 'nlags' as appropriate
    print('BG test statistic:', bg_test[0])
    print('BG test p-value:', bg_test[1])
    # Interpretation of Breusch-Godfrey test
    if bg_test[1] < 0.05:
        print("The null hypothesis of no autocorrelation is rejected, suggesting autocorrelation.")
    else:
        print("The null hypothesis of no autocorrelation cannot be rejected.")
    # Independent Errors Tests Summary and Recommendations
    print("\nIndependent Errors Tests Summary and Recommendations:")

    # Initialize a flag for autocorrelation issues
    autocorrelation_flag = False
    # Summary of the outcomes
    if dw_statistic < 1.5 or dw_statistic > 2.5:
        autocorrelation_flag = True
        print("- Durbin-Watson test indicates autocorrelation.")
    if bg_test[1] < 0.05:
        autocorrelation_flag = True
        print("- Breusch-Godfrey test indicates autocorrelation.")
    # Final conclusion and suggestions
    if autocorrelation_flag:
        print("\nThere is evidence of autocorrelation in the model residuals.")
        print("Suggestions to address autocorrelation include:")
        print("- Adding lags of the dependent variable or other time-related variables if the data is time series.")
        print("- Considering the addition of omitted variables that could explain the autocorrelation.")
        print("- Using Generalized Least Squares or Newey-West standard errors to correct for autocorrelation.")
        print("- If the model is a time series, considering ARIMA models or other time series specific methods.")
    else:
        print("\nNo significant autocorrelation detected in the model residuals.")

    ####### 4 Normality Test
    print("\n4. Normal Errors:")
    print("To assess the normality of residuals, the Shapiro-Wilk test can be used.")
    print("Null hypothesis (H0): The data is normally distributed. Residuals are normally distributed")
    print("Alternative hypothesis (H1): The data is not normally distributed. Residuals are not normally distributed")
    # Initialize a flag to track normality issues
    normality_issues = False
    # Conducting the Shapiro-Wilk test on residuals
    shapiro_statistic, shapiro_p_value = stats.shapiro(residuals)
    # Output the test statistic and p-value
    print(f'Shapiro-Wilk test statistic: {shapiro_statistic}')
    print(f'Shapiro-Wilk test p-value: {shapiro_p_value}')
    # Interpretation of the results
    if shapiro_p_value < 0.05:
        print("The null hypothesis of normality is rejected. This suggests the residuals are not normally distributed.")
        normality_issues = True
    else:
        print("The null hypothesis of normality cannot be rejected. This suggests the residuals are normally distributed.")
    # Generate Q-Q plot for residuals
    fig = plt.figure()
    stats.probplot(residuals, plot=plt)
    plt.title('Q-Q Plot')
    plt.suptitle(f'Shapiro-Wilk p-value: {shapiro_p_value:.4f}')
    # Show the plot
    plt.show()
    # Conducting the KS test on residuals
    print("\nKolmogorov-Smirnov Test (Normal Errors)")
    print("Null Hypothesis (H0): The sample distribution is the same as the reference distribution.")
    print("Alternative Hypothesis (H1): The sample distribution is different from the reference distribution.")
    ks_statistic, ks_p_value = kstest(residuals, 'norm')
    print('KS test statistic:', ks_statistic)
    print('KS test p-value:', ks_p_value)
    # Interpretation of the KS test results
    if ks_p_value < 0.05:
        print("The null hypothesis of normality is rejected based on the KS test.")
        print("This suggests the residuals are not normally distributed.")
        normality_issues = True
    else:
        print("The null hypothesis of normality cannot be rejected based on the KS test.")
        print("This suggests the residuals are normally distributed.")
    # Conducting the Anderson-Darling test on residuals
    ad_test = anderson(residuals, dist='norm')
    print('\nAnderson-Darling test statistic:', ad_test.statistic)
    print('Null Hypothesis (H0): The sample comes from a population that follows the specified distribution.')
    print('Alternative Hypothesis (H1): The sample does not come from a population that follows the specified distribution.')
    print('AD test critical values:', ad_test.critical_values)
    print('AD test significance levels:', ad_test.significance_level)
    # Interpretation of the results
    if ad_test.statistic > ad_test.critical_values[2]:  # Typically, index 2 is for the 5% level
        print("The null hypothesis of normality is rejected at the 5% level based on the AD test. This suggests the residuals are not normally distributed.")
        normality_issues = True
    else:
        print("The null hypothesis of normality cannot be rejected at the 5% level based on the AD test. This suggests the residuals are normally distributed.")

    # Conclusion and Recommendations
    print("\n Conclusion")
    if normality_issues:
        print("At least one normality test indicates that the residuals are not normally distributed.")
        print("Recommendations:")
        print("- Consider using transformations (e.g., log, square root) to normalize residuals.")
        print("- If transformations do not work, consider using non-parametric models or robust regression methods.")
        print("- Reassess the model: ensure that all relevant variables are included and that the model is correctly specified.")
    else:
        print("All normality tests agree that there is no evidence to suggest that the residuals are not normally distributed.")
        print("The assumption of normality is satisfied for this model.")
   

    ####### 5 Multicollinearity Test
    print("\n5. No Multicollinearity:")
    print("The Variance Inflation Factor (VIF) is used to detect multicollinearity.")

    # Add a constant for the intercept
    X_const = add_constant(X)

    # Calculating VIF for each feature
    VIF_df = pd.DataFrame()
    VIF_df['feature'] = X_const.columns
    VIF_df['VIF'] = [variance_inflation_factor(X_const.values, i) for i in range(X_const.shape[1])]

    # Display the VIF results
    print(VIF_df)
    # Bar plot for VIF values
    plt.figure(figsize=(12, 6))
    plt.bar(VIF_df['feature'], VIF_df['VIF'], color='skyblue')
    plt.title('Variance Inflation Factor (VIF) for each variable')
    plt.xlabel('Features')
    plt.ylabel('VIF Value')
    plt.axhline(10, color='red', linestyle='--', label='VIF Threshold = 10')
    plt.legend()
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()
    # Interpretation of the VIF results
    for vif in VIF_df.itertuples():
        if vif.VIF < 5:
            print(f"{vif.feature} has a VIF of {vif.VIF}, suggesting no significant multicollinearity.")
        elif 5 <= vif.VIF < 10:
            print(f"{vif.feature} has a VIF of {vif.VIF}, suggesting moderate multicollinearity.")
        else:
            print(f"{vif.feature} has a VIF of {vif.VIF}, suggesting high multicollinearity.")
    
    
